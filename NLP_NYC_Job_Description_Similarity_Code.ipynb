{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Jupyter Notebook uses NLP and unsupervised learning to classify jobs based on their job descrition\n",
    "# The data from this project comes from a publicly available CSV on Kaggle\n",
    "# Kaggle URL: https://www.kaggle.com/new-york-city/new-york-city-current-job-postings?select=nyc-jobs.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block imports python libraries\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from IPython.core.display import display, HTML\n",
    "import pickle\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import os\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Lasso, LassoCV, Ridge, RidgeCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import statsmodels.api as sm\n",
    "%matplotlib inline\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block reads the CSV file and renders the data in a Pandas Dataframe\n",
    "\n",
    "# To create a dataframe with this data, download the CSV file from the Kaggle URl referenced at the top\n",
    "# Replace the file path in the line below with your own CSV file path\n",
    "NYC_Jobs_DF = pd.read_csv('/Your_File_Path/Folder_With_This_CSV/nyc_jobs.csv')\n",
    "\n",
    "#display(NYC_Jobs_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: There are duplicates in this data set (i.e. more than 1 line of data having the same job description)\n",
    "# If there were a few jobs with huge numbers of duplicates it could have an effect on the clustering:\n",
    "\n",
    "# Example: A job with 100 duplicates in a dataset where each other job has only 1 row would strongly tend to keep \n",
    "#          the centroid of its cluster near iself.  If the repeat job description is such that is should be at the \n",
    "#          edge of a sparse cluster, it will increase the probability of excluding jobs on the opposite side of\n",
    "#          its cluster, while incorporating jobs that should be on the edge of neighboring clusters.\n",
    "\n",
    "# This effect is not so strong for this data set, as most duplicated jobs have only 2 rows, and the job with the\n",
    "# most duplications has 14.  Thus, we can have reasonable clustering without first removing the duplicates.\n",
    "# (Though it is possible that duplicates are helping keep the Taxi and Limosine commission jobs in their own group)\n",
    "\n",
    "# The effect will tend to be increased if one or a few entries have many duplicates (as opposed to duplication \n",
    "# being spread out).  The effect also would tend to increase as the number of clusters goes up and tends to \n",
    "# decrease as the size of the data set increases.\n",
    "\n",
    "# A version without duplicates might be uploaded onto GitHub later for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block checks for information on duplicates\n",
    "# Specifically, it checks for dulicates there are which occur above a certain frequency level (set by the user)\n",
    "\n",
    "# This code block is really for EDA, so you do not actually need to run it to proceed to the rest of the analysis\n",
    "# But you can if you want to, I won't stop you\n",
    "\n",
    "# (I don't recommend it personally, since you will just be looking through a lot of tedious stuff, but whatever, you do you)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Minimum_Times_Duplicated_To_Display = 3 # This variable establishes the minimum times an entry must repeat to show\n",
    "# For example, if the value is 3, only job descriptions repeated 3 or more times will be shown when this code runs\n",
    "# If you lower it to 2 there will be many more results that will indicate they occurred twice in this data set\n",
    "# This code will not display any job description that only appears once, even if you set the value to 1 or less\n",
    "# This only counts exact matches as duplicates, not merely close jobs\n",
    "\n",
    "\n",
    "First_Occurance_List = []\n",
    "Duplicate_Occurances_Dict = {}\n",
    "\n",
    "Index = 0\n",
    "\n",
    "while(Index < len(NYC_Jobs_DF)):\n",
    "\n",
    "    Current_Item = NYC_Jobs_DF['Job Description'][Index]\n",
    "\n",
    "    if(Current_Item in First_Occurance_List):\n",
    "        if(Current_Item in Duplicate_Occurances_Dict):\n",
    "            Duplicate_Occurances_Dict[Current_Item] += 1\n",
    "        else:\n",
    "            Duplicate_Occurances_Dict[Current_Item] = 2\n",
    "    else:\n",
    "        First_Occurance_List.append(Current_Item)\n",
    "\n",
    "    Index += 1\n",
    "    \n",
    "Number_Results_Showing = 0\n",
    "\n",
    "for Description in Duplicate_Occurances_Dict:\n",
    "\n",
    "    if(Duplicate_Occurances_Dict[Description] >= Minimum_Times_Duplicated_To_Display):\n",
    "\n",
    "        print(\" \")\n",
    "        print(\"This job description occurs\", Duplicate_Occurances_Dict[Description], \"times in this data set:\")\n",
    "        print(\" \")\n",
    "        print(Description)\n",
    "        print(\" \")\n",
    "        print(\"-----------------------------------------------------------------------------------------\")\n",
    "        Number_Results_Showing += 1\n",
    "        \n",
    "    if(Minimum_Times_Duplicated_To_Display < 3):\n",
    "        time.sleep(0.01) # This is needed to avoid exceeding the data rate limit for Jupyter notebooks\n",
    "        \n",
    "if(Number_Results_Showing == 1):\n",
    "    print(\"Showing 1 result\")\n",
    "else:\n",
    "    print(\"Showing\", Number_Results_Showing, \"results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block creates a function to replace certain punctuation in a text string with spaces (\" \")\n",
    "# This is needed because job descriptions will later be broken into words by splitting on space characters\n",
    "\n",
    "# The punctuation to be removed are characters that would be expected to be at the beginning or end of a word\n",
    "# Example: a sentence that ends \"...this job.\" should count \"job.\" as the same word as \"job\" occurring mid-sentence\n",
    "\n",
    "# Apostrophes are allowed to remain in the string, since they would tend to occur in the middle of a word\n",
    "# Example: supose the phrase \"the administrator's role\" occurs \n",
    "# This should not count as one instance of the word \"administrator\" plus one instance of the word \"s\"\n",
    "\n",
    "def replace_punctuation_with_spaces(input_string):\n",
    "    \n",
    "    Punctuation_List = [\",\", \":\", \"-\", \";\", \".\", \"!\", \"?\", \"/\"]\n",
    "                        \n",
    "    Output_String = \"\"\n",
    "    \n",
    "    Index = 0\n",
    "                        \n",
    "    for char in input_string:\n",
    "        if char in Punctuation_List:\n",
    "            Output_String += \" \"\n",
    "        elif(char == \"'\\'\"):\n",
    "            Output_String += \" \"\n",
    "        else:\n",
    "            Output_String += char\n",
    "            \n",
    "    return Output_String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block creates a function to convert strings to uppercase letters only\n",
    "# It also allows apostrophes in the original string to remain\n",
    "\n",
    "def uppercase_letters_and_apostrophes(input_string):\n",
    "    \n",
    "    Capital_List = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \n",
    "                    \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\", \"'\"]\n",
    "    \n",
    "    Lowercase_List = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \n",
    "                      \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\"]\n",
    "    \n",
    "    Output_String = \"\"\n",
    "    \n",
    "    Letter_Index = 0\n",
    "    \n",
    "    for char in input_string:\n",
    "        if(char in Capital_List):\n",
    "            Output_String += char\n",
    "        elif(char in Lowercase_List):\n",
    "            Letter_Index = Lowercase_List.index(char)\n",
    "            Output_String += (Capital_List[Letter_Index])\n",
    "            \n",
    "    return Output_String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block creates a function to convert strings to uppercase letters only\n",
    "# It also allows spaces and apostrophes in the original string to remain\n",
    "\n",
    "def uppercase_letters_spaces_and_apostrophes(input_string):\n",
    "    \n",
    "    Capital_List = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \n",
    "                    \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\", \"'\", \" \"]\n",
    "    \n",
    "    Lowercase_List = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \n",
    "                      \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\"]\n",
    "    \n",
    "    Output_String = \"\"\n",
    "    \n",
    "    Letter_Index = 0\n",
    "    \n",
    "    for char in input_string:\n",
    "        if(char in Capital_List):\n",
    "            Output_String += char\n",
    "        elif(char in Lowercase_List):\n",
    "            Letter_Index = Lowercase_List.index(char)\n",
    "            Output_String += (Capital_List[Letter_Index])\n",
    "            \n",
    "    return Output_String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block creates a list of jobs descriptions capitalized and with most punctuation removed\n",
    "\n",
    "List_of_Clean_Job_Descriptions = []\n",
    "\n",
    "Index = 0\n",
    "\n",
    "while(Index < len(NYC_Jobs_DF)):\n",
    "    \n",
    "    Description_String = replace_punctuation_with_spaces(NYC_Jobs_DF['Job Description'][Index])\n",
    "    List_of_Words_In_This_Description = list(Description_String.split(\" \"))\n",
    "    \n",
    "    List_of_Clean_Job_Descriptions.append(uppercase_letters_spaces_and_apostrophes(Description_String))\n",
    "    \n",
    "    Index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block uses the list created in the previous block to create a new column in the dataframe\n",
    "\n",
    "NYC_Jobs_DF['Job Description Cleaned'] = List_of_Clean_Job_Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block creates a word stemmer so, for example, \"inspector\" and \"inspectors\" will count as the same word\n",
    "\n",
    "Stemmer_1 = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block applies the stemmer to the job descriptions\n",
    "\n",
    "List_Of_Stemmed_Descriptions = []\n",
    "\n",
    "Index = 0\n",
    "\n",
    "while(Index < len(List_of_Clean_Job_Descriptions)):\n",
    "    \n",
    "    Desc = List_of_Clean_Job_Descriptions[Index]\n",
    "    Desc_List = list((Desc).split(\" \"))\n",
    "    \n",
    "    for word in Desc_List:\n",
    "        word = Stemmer_1.stem(word)\n",
    "        \n",
    "    List_Of_Stemmed_Descriptions.append(Desc)\n",
    "    \n",
    "    Index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block creates a column in the dataframe for the job descriptions that have had the stemmer applied\n",
    "\n",
    "NYC_Jobs_DF['Job Description Stemmed Cleaned'] = List_Of_Stemmed_Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block shows an example of a pre-processed job description\n",
    "# This can be compared to the same one after processing in the code block below\n",
    "# To view this description, de-commentify the code line below\n",
    "\n",
    "NYC_Jobs_DF['Job Description'][35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block shows an example of a processed job description\n",
    "# This can be compared to the same one from before processing in the code block above\n",
    "# To view this description, de-commentify the code line below\n",
    "\n",
    "NYC_Jobs_DF['Job Description Stemmed Cleaned'][35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block creates a TFIDF Vectorizer\n",
    "# This will be used to compare similarities in word usage in jobs descriptions\n",
    "# This method will also correct for differences in length between different job descriptions\n",
    "# The default (sum of squares) method is used here, which leaves all normalized vectors the same length\n",
    "# Correcting for length will be important later as jobs will judged similar or not based on Euclidean Distance\n",
    "# In the absence of corrections for length, it would be better to use Cosine Distance\n",
    "\n",
    "TFDIF_1 = TfidfVectorizer(stop_words='english')\n",
    "TFDIF_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block creates a sparse matrix of the tokenized words from the TFDIF Vectorizer\n",
    "# The sparse matrix keeps track of the frequency of word usage for each domcument\n",
    "\n",
    "X_Full = TFDIF_1.fit_transform(NYC_Jobs_DF['Job Description Stemmed Cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block creates a Dataframe of the TFIDF-adjusted job description vectors\n",
    "# Each tokenized word is a column in the dataframe \n",
    "# Each tokenized word will be represented as a dimension in Euclidean space for K-Means clustering\n",
    "\n",
    "TFIDF1_DF_Full = pd.DataFrame(X_Full.toarray(),columns=TFDIF_1.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next code block may take a long time to run, as it is performing 13 iterations of K-Means Clustering\n",
    "# This was used to find the number of clusters that had the highest silhouette score\n",
    "\n",
    "# You can skip the next 3 code blocks to save time (the 2 after depent on the output of the next block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block uses K-Means clustering to group the jobs that have the most similar descriptions\n",
    "# This block performs 13 clusterings, with the number of groups ranging from 2 to 14\n",
    "# This block also scores each custering using the silhouette_score for comparison\n",
    "\n",
    "Current_Cluster_Number = 2\n",
    "\n",
    "Silhouette_Score_List = []\n",
    "\n",
    "while((Current_Cluster_Number < 15)):\n",
    "\n",
    "    k_means_cluster_loop_full_tfidf = KMeans(n_clusters=Current_Cluster_Number, random_state=11)\n",
    "    k_means_cluster_loop_full_tfidf.fit(TFIDF1_DF_Full)\n",
    "    \n",
    "    Cluster_Labels = k_means_cluster_loop_full_tfidf.labels_\n",
    "    \n",
    "    Silhouette_Score = silhouette_score(TFIDF1_DF_Full, Cluster_Labels, metric='euclidean', sample_size=None, random_state=None)\n",
    "    \n",
    "    Silhouette_Score_List.append(Silhouette_Score)\n",
    "    \n",
    "    Current_Cluster_Number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to see the Silhouette Scores in list form, de-commentify the print statement at the bottom\n",
    "\n",
    "print(Silhouette_Score_List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block shows a graph of the Silouette Scores for each cluster size\n",
    "\n",
    "K_Numbers = [2,3,4,5,6,7,8,9,10,11,12,13,14]\n",
    "plt.bar(K_Numbers, Silhouette_Score_List);\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Average Silhouette Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# NOTE: I obtained my results for silhouette score, and subsequent category generation, by running this code on\n",
    "# a MacBook Pro running MacOS Mojave version 10.14.6, and with Python 3.7 - using the given (pseudo) random state.\n",
    "\n",
    "# If you use a different computer, operating system, or Python version, you may get different results even using\n",
    "# the same random state number.\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block uses the number of clusters which yielded the highest Silhouette score to generate labels\n",
    "\n",
    "k_means_cluster_full_tfidf = KMeans(n_clusters=12, random_state=11)\n",
    "k_means_cluster_full_tfidf.fit(TFIDF1_DF_Full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block creates seperate lists for job descriptions of each category of job\n",
    "# It also creates seperate lists for the index (row) number of each category\n",
    "# Both types of lists are updated in the same order and the lists are not altered later\n",
    "# This allows for the use of index numbers to search only for other jobs in the same category\n",
    "\n",
    "# Labels of \"0\" through \"11\" are automatically generated for each job by the model to indicate job's group\n",
    "\n",
    "Index = 0\n",
    "\n",
    "List_0 = [] # There lists will hold the job description vectors\n",
    "List_1 = []\n",
    "List_2 = []\n",
    "List_3 = []\n",
    "List_4 = []\n",
    "List_5 = []\n",
    "List_6 = []\n",
    "List_7 = []\n",
    "List_8 = []\n",
    "List_9 = []\n",
    "List_10 = []\n",
    "List_11 = []\n",
    "\n",
    "Ind_List_0 = [] # There lists will hold the Index numbers, in the same order as the vectors\n",
    "Ind_List_1 = []\n",
    "Ind_List_2 = []\n",
    "Ind_List_3 = []\n",
    "Ind_List_4 = []\n",
    "Ind_List_5 = []\n",
    "Ind_List_6 = []\n",
    "Ind_List_7 = []\n",
    "Ind_List_8 = []\n",
    "Ind_List_9 = []\n",
    "Ind_List_10 = []\n",
    "Ind_List_11 = []\n",
    "\n",
    "while(Index < len(k_means_cluster_full_tfidf.labels_)):\n",
    "    \n",
    "    if(k_means_cluster_full_tfidf.labels_[Index] == 0): # Jobs assigned label 0 appended to these lists\n",
    "        List_0.append(NYC_Jobs_DF[\"Job Description\"][Index])\n",
    "        Ind_List_0.append(Index)\n",
    "    elif(k_means_cluster_full_tfidf.labels_[Index] == 1): # Jobs assigned label 1 appended to these lists\n",
    "        List_1.append(NYC_Jobs_DF[\"Job Description\"][Index])\n",
    "        Ind_List_1.append(Index)\n",
    "    elif(k_means_cluster_full_tfidf.labels_[Index] == 2): # Etc\n",
    "        List_2.append(NYC_Jobs_DF[\"Job Description\"][Index])\n",
    "        Ind_List_2.append(Index)\n",
    "    elif(k_means_cluster_full_tfidf.labels_[Index] == 3):\n",
    "        List_3.append(NYC_Jobs_DF[\"Job Description\"][Index])\n",
    "        Ind_List_3.append(Index)\n",
    "    elif(k_means_cluster_full_tfidf.labels_[Index] == 4):\n",
    "        List_4.append(NYC_Jobs_DF[\"Job Description\"][Index])\n",
    "        Ind_List_4.append(Index)\n",
    "    elif(k_means_cluster_full_tfidf.labels_[Index] == 5):\n",
    "        List_5.append(NYC_Jobs_DF[\"Job Description\"][Index])\n",
    "        Ind_List_5.append(Index)\n",
    "    elif(k_means_cluster_full_tfidf.labels_[Index] == 6):\n",
    "        List_6.append(NYC_Jobs_DF[\"Job Description\"][Index])\n",
    "        Ind_List_6.append(Index)\n",
    "    elif(k_means_cluster_full_tfidf.labels_[Index] == 7):\n",
    "        List_7.append(NYC_Jobs_DF[\"Job Description\"][Index])\n",
    "        Ind_List_7.append(Index)\n",
    "    elif(k_means_cluster_full_tfidf.labels_[Index] == 8):\n",
    "        List_8.append(NYC_Jobs_DF[\"Job Description\"][Index])\n",
    "        Ind_List_8.append(Index)\n",
    "    elif(k_means_cluster_full_tfidf.labels_[Index] == 9):\n",
    "        List_9.append(NYC_Jobs_DF[\"Job Description\"][Index])\n",
    "        Ind_List_9.append(Index)\n",
    "    elif(k_means_cluster_full_tfidf.labels_[Index] == 10):\n",
    "        List_10.append(NYC_Jobs_DF[\"Job Description\"][Index])\n",
    "        Ind_List_10.append(Index)\n",
    "    elif(k_means_cluster_full_tfidf.labels_[Index] == 11):\n",
    "        List_11.append(NYC_Jobs_DF[\"Job Description\"][Index])\n",
    "        Ind_List_11.append(Index)\n",
    "    \n",
    "    Index += 1\n",
    "\n",
    "# The category names in the comments next to each list were assigned based on spot checking of each list's jobs\n",
    "# If you want to see the number of jobs in each category, de-commentify the print statements below\n",
    "\n",
    "                    # List # : Types of jobs\n",
    "                    #----------------------------------------------------------------\n",
    "                           #:\n",
    "#print(len(List_0)) # List_0: Public Works Inspection, Compiance, and Quality Assurance\n",
    "#print(len(List_1)) # List_1: Miscellaneous\n",
    "#print(len(List_2)) # List_2: Administrative, Finance, Budget, and Analysis\n",
    "#print(len(List_3)) # List_3: Housing Administration and Project Management\n",
    "#print(len(List_4)) # List_4: Health Services, Education, and Family/Youth/Child Welfare\n",
    "#print(len(List_5)) # List_5: Water Quality Assurance and Wastewater Facilities Inpection/Admin\n",
    "#print(len(List_6)) # List_6: Law/Legal\n",
    "#print(len(List_7)) # List_7: Procurement, Contracting, and Planning\n",
    "#print(len(List_8)) # List_8: Engineering, Construction, and Safety Inspections \n",
    "#print(len(List_9)) # List_9: Taxi and Limousine Commision\n",
    "#print(len(List_10)) # List_10: City Planning and Land Use/Zoning\n",
    "#print(len(List_11)) # List_11: Investigations and Record Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block creates a list of lists for both the job vector lists and the index numbers\n",
    "# The order is preserved such that, for any integers X and Y (where X < 12 and Y < (length of this dataframe)):\n",
    "\n",
    "# List_Of_K_Clusters[X][Y] refers to the job on the same row as List_Of_Indexes[X][Y]\n",
    "\n",
    "# In addition, the (0 to 11) labels assigned by the models match the index numbers of their corresponding lists\n",
    "\n",
    "List_Of_K_Clusters = [List_0, List_1, List_2, List_3, List_4, List_5,\n",
    "                      List_6, List_7, List_8, List_9, List_10, List_11]\n",
    "\n",
    "List_Of_Indexes = [Ind_List_0, Ind_List_1, Ind_List_2, Ind_List_3, Ind_List_4, Ind_List_5,\n",
    "                   Ind_List_6, Ind_List_7, Ind_List_8, Ind_List_9, Ind_List_10, Ind_List_11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get a sample of each category, you can run this code block\n",
    "# You can adjust the Samples_Per_Category parameter to change the number of jobs seen from each cluster\n",
    "\n",
    "# This is to enable human-understandable category labelling, not something necessary for later code blocks to work\n",
    "# You do not need to run it to continue with the analysis, but you can if you want\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------\n",
    "\n",
    "Meta_Index = 0\n",
    "Samples_Per_Category = 6 # Adjust this to alter the number of jobs in each category you sample\n",
    "\n",
    "while(Meta_Index < len(List_Of_K_Clusters)):\n",
    "    \n",
    "    Current_List = List_Of_K_Clusters[Meta_Index]\n",
    "    Increment = int((len(Current_List)//Samples_Per_Category))\n",
    "    Index = 0\n",
    "    Jobs_Printed = 0\n",
    "    \n",
    "    print(\" \")\n",
    "    print(\"----------------------------------------------------------------------------------------\")\n",
    "    print(\" \")\n",
    "    print(\"                      CATEGORY NUMBER: \", Meta_Index)\n",
    "    print(\" \")\n",
    "    print(\"----------------------------------------------------------------------------------------\")\n",
    "    print(\" \")\n",
    "    \n",
    "    while(Jobs_Printed < Samples_Per_Category):\n",
    "        \n",
    "        print(\" \")\n",
    "        print(Current_List[Index])\n",
    "        print(\"----------------------------------------------------------------------------------------\")\n",
    "        print(\" \")\n",
    "        Index += Increment\n",
    "        Jobs_Printed += 1\n",
    "                    \n",
    "    Meta_Index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block creates a function to enable a user to input a job and see similar jobs from the same category\n",
    "# By default it prints 5 results and excludes exact duplicates, though a user can change this\n",
    "\n",
    "def most_similar_in_this_category(Test_Case, number_of_results_requested=5, allow_exact_duplicates=False, \n",
    "                                  allow_results_outside_category=False):\n",
    "    \n",
    "    \"\"\"This function takes a TFIDF-transformed vector from a job desciption and returns \n",
    "    the original text of other job descriptions that are similar based on Euclidean \n",
    "    distance.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    Test_Case : Pandas Series with shape (10579,) which corresponds to the shape \n",
    "        of description vectors for this data set\n",
    "                        \n",
    "    number_of_results_requested : Integer (default value 5) determines the number of \n",
    "        results that will be returned\n",
    "                                 \n",
    "    allow_exact_duplicates : Boolean (default value False) determines whether to \n",
    "        allow results that are verbatim identical to the input or to each other\n",
    "                            \n",
    "    allow_results_outside_category : Boolean (default value False) determines whether \n",
    "        to allow results that outside the cluster of the input job vector\n",
    "                                    \n",
    "                                    \"\"\"\n",
    "\n",
    "    Closest = []\n",
    "    Min_Dist_List = []\n",
    "    Exclude_List = []\n",
    "    \n",
    "    if(allow_results_outside_category == False): # This is if you are only searching within the same cluster\n",
    "        \n",
    "        Meta_Index = (k_means_cluster_full_tfidf.predict([Test_Case]))[0]\n",
    "        List_To_Search = List_Of_K_Clusters[Meta_Index] # This refers the program to the list for the input\n",
    "        Index_List_To_Search = List_Of_Indexes[Meta_Index]\n",
    "\n",
    "        Index = 0\n",
    "\n",
    "        while(Index < len(List_To_Search)):\n",
    "\n",
    "            TFIDF_Index = Index_List_To_Search[Index] \n",
    "\n",
    "            Element = TFIDF1_DF_Full.iloc[TFIDF_Index]\n",
    "\n",
    "            if((allow_exact_duplicates == False) and ((Element == Test_Case).all())):\n",
    "                pass # Indicates a duplicate to be excluded\n",
    "            elif(NYC_Jobs_DF[\"Job Description\"][TFIDF_Index] in Exclude_List):\n",
    "                pass # Indicates a duplicate to be excluded\n",
    "            else:\n",
    "\n",
    "                Euclidean_Distance = np.linalg.norm(Test_Case - Element)\n",
    "\n",
    "                if(len(Closest) < number_of_results_requested): # This part adds a result if the minimum number of results has not been reached yet\n",
    "                    Closest.append(NYC_Jobs_DF[\"Job Description\"][TFIDF_Index])\n",
    "                    Min_Dist_List.append(Euclidean_Distance)\n",
    "                    if(allow_exact_duplicates == False):\n",
    "                        Exclude_List.append(NYC_Jobs_DF[\"Job Description\"][TFIDF_Index])\n",
    "                elif(Euclidean_Distance < max(Min_Dist_List)): # If the minimum number of results has been reached, this part checks if the current result is a closer match \n",
    "                    Remove_Index = Min_Dist_List.index(max(Min_Dist_List)) # If the current result is a closer match, it replaces the most distant match on the list\n",
    "                    Closest.pop(Remove_Index)\n",
    "                    Min_Dist_List.pop(Remove_Index)\n",
    "                    Closest.append(NYC_Jobs_DF[\"Job Description\"][TFIDF_Index])\n",
    "                    Min_Dist_List.append(Euclidean_Distance)\n",
    "                    if(allow_exact_duplicates == False):\n",
    "                        Exclude_List.append(NYC_Jobs_DF[\"Job Description\"][TFIDF_Index])\n",
    "\n",
    "            Index += 1\n",
    "            \n",
    "    else: # This is if you are only searching the entire data set for matches, not just within the same cluster     \n",
    "          # For this data set, that categories are somewhat artificial, and can change based random state starting conditions\n",
    "          # Thus, it may be advantageous to search the whole data set, (since this data set also is not that big)\n",
    "          # However, if you use something like this on a much larger data set with more clear categories, it can\n",
    "          # save a lot of time to have the data pre-clustered and search only with the input's group\n",
    "          \n",
    "        TFIDF_Index = 0\n",
    "        \n",
    "        while(TFIDF_Index < len(NYC_Jobs_DF)):\n",
    "\n",
    "            Element = TFIDF1_DF_Full.iloc[TFIDF_Index]\n",
    "\n",
    "            if((allow_exact_duplicates == False) and ((Element == Test_Case).all())):\n",
    "                pass\n",
    "            elif(NYC_Jobs_DF[\"Job Description\"][TFIDF_Index] in Exclude_List):\n",
    "                pass\n",
    "            else:\n",
    "\n",
    "                Euclidean_Distance = np.linalg.norm(Test_Case - Element)\n",
    "\n",
    "                if(len(Closest) < number_of_results_requested):\n",
    "                    Closest.append(NYC_Jobs_DF[\"Job Description\"][TFIDF_Index])\n",
    "                    Min_Dist_List.append(Euclidean_Distance)\n",
    "                    if(allow_exact_duplicates == False):\n",
    "                        Exclude_List.append(NYC_Jobs_DF[\"Job Description\"][TFIDF_Index])\n",
    "                elif(Euclidean_Distance < max(Min_Dist_List)):\n",
    "                    Remove_Index = Min_Dist_List.index(max(Min_Dist_List))\n",
    "                    Closest.pop(Remove_Index)\n",
    "                    Min_Dist_List.pop(Remove_Index)\n",
    "                    Closest.append(NYC_Jobs_DF[\"Job Description\"][TFIDF_Index])\n",
    "                    Min_Dist_List.append(Euclidean_Distance)\n",
    "                    if(allow_exact_duplicates == False):\n",
    "                        Exclude_List.append(NYC_Jobs_DF[\"Job Description\"][TFIDF_Index])\n",
    "            \n",
    "            TFIDF_Index += 1\n",
    "        \n",
    "    if(len(Closest) < number_of_results_requested):\n",
    "        print(\"We could only find\", (len(Closest)), \"other jobs in this category.\")\n",
    "        \n",
    "    Results_Printed = 0\n",
    "\n",
    "    for Desc in Closest:\n",
    "        \n",
    "        Results_Printed += 1\n",
    "        print(\" \")\n",
    "        print(\"Result Number\", Results_Printed)\n",
    "        print(\" \")\n",
    "        print(Desc)\n",
    "        print(\" \")\n",
    "        print(\"--------------------------------------------------------------------------------------------------------\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block creates a test case for the function\n",
    "\n",
    "Test_Case_1 = TFIDF1_DF_Full.iloc[29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block uses the most_similar_in_this_category() function to find similar jobs to the test case job\n",
    "\n",
    "most_similar_in_this_category(Test_Case_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block views the job description that corresponds to test case\n",
    "# This allows users to compare the results with the input to see how well they match\n",
    "\n",
    "NYC_Jobs_DF[\"Job Description\"][29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block creates a function to choose a job at random and then get results of jobs similar to it\n",
    "# This allows users to check performance of the most_similar_in_this_category() function on a range of jobs\n",
    "\n",
    "def pick_random_job_and_check_for_similar(show_category=False):\n",
    "    \n",
    "    Job_Index = int((len(NYC_Jobs_DF) * random.random())//1)\n",
    "    \n",
    "    if(show_category == True):\n",
    "        \n",
    "        Category = k_means_cluster_full_tfidf.labels_[Job_Index]\n",
    "        print(\" \")\n",
    "        print(\"        CATEGORY NUMBER: \", Category)\n",
    "        print(\" \")\n",
    "        print(\"--------------------------------------------------------------------------------------------------------\")\n",
    "    \n",
    "    print(\" \")\n",
    "    print(\"Random job selected to find matches for:\")\n",
    "    print(\" \")\n",
    "    print(NYC_Jobs_DF[\"Job Description\"][Job_Index])\n",
    "    print(\" \")\n",
    "    print(\"--------------------------------------------------------------------------------------------------------\")  \n",
    "    \n",
    "    most_similar_in_this_category(TFIDF1_DF_Full.iloc[Job_Index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can call this function just by running the code block, it does require take any arguments\n",
    "# However, you can optionally show the category number by putting \"show_category=True\" in the parenthesis\n",
    "\n",
    "pick_random_job_and_check_for_similar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
